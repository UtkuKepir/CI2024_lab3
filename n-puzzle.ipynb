{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import heapq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 3\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_test(state):\n",
    "    return all(state.flatten() == list(range(1, PUZZLE_DIM**2)) + [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth-First Search with Bound\n",
    "def dfs_bound(state, bound, path, visited):\n",
    "    states_evaluated = 1  \n",
    "\n",
    "    if goal_test(state):  \n",
    "        return path, state, states_evaluated\n",
    "\n",
    "    if len(path) > bound: \n",
    "        return None, None, states_evaluated\n",
    "\n",
    "    visited.add(state.tobytes())  \n",
    "\n",
    "    for action in available_actions(state):  \n",
    "        new_state = do_action(state, action)\n",
    "        if new_state.tobytes() not in visited:  \n",
    "            result, final_state, sub_states_evaluated = dfs_bound(new_state, bound, path + [action], visited)\n",
    "            states_evaluated += sub_states_evaluated  \n",
    "            if result:  \n",
    "                return result, final_state, states_evaluated\n",
    "\n",
    "    visited.remove(state.tobytes())  \n",
    "    return None, None, states_evaluated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth-First is Not-Complete without Iterative Deepening\n",
    "def iterative_deepening_dfs(state, max_depth=60):\n",
    "    total_states_evaluated  = 0  \n",
    "\n",
    "    for depth in range(1, max_depth + 1):  \n",
    "            visited = set()\n",
    "            print(f\"Searching with depth limit: {depth}\")\n",
    "            result, final_state, states_evaluated = dfs_bound(state, depth, [], visited)\n",
    "            total_states_evaluated += states_evaluated  \n",
    "            if result:  \n",
    "                quality = len(result)  \n",
    "                return final_state, result, quality, total_states_evaluated\n",
    "\n",
    "    return None, None, None, total_states_evaluated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breadth-First Search\n",
    "def bfs(state):\n",
    "    frontier = deque([(state, [])])\n",
    "    visited = set()\n",
    "    visited.add(state.tobytes())\n",
    "    states_evaluated = 0\n",
    "    while frontier:\n",
    "        current_state, path = frontier.popleft()\n",
    "        states_evaluated += 1\n",
    "\n",
    "        if goal_test(current_state):\n",
    "            quality = len(path)\n",
    "            cost = states_evaluated\n",
    "            return current_state, path, quality, cost # final state\n",
    "\n",
    "        for action in available_actions(current_state):\n",
    "            new_state = do_action(current_state, action)\n",
    "            if new_state.tobytes() not in visited:\n",
    "                visited.add(new_state.tobytes())\n",
    "                frontier.append((new_state, path + [action]))\n",
    "                \n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming distance heuristic for A* search\n",
    "def h_hamming(state):\n",
    "    goal = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "    return np.sum(state != goal) - 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manhattan distance heuristic for A* search\n",
    "def h_manhattan(state):\n",
    "    goal = {val: (i // PUZZLE_DIM, i % PUZZLE_DIM) for i, val in enumerate(range(1, PUZZLE_DIM**2))}\n",
    "    goal[0] = (PUZZLE_DIM - 1, PUZZLE_DIM - 1)\n",
    "    distance = 0\n",
    "    for x in range(PUZZLE_DIM):\n",
    "        for y in range(PUZZLE_DIM):\n",
    "            val = state[x, y]\n",
    "            gx, gy = goal[val]\n",
    "            distance += abs(x - gx) + abs(y - gy)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Conflict heuristic for A* search\n",
    "def h_linear_conflict(state):\n",
    "    goal_positions = {val: (i // PUZZLE_DIM, i % PUZZLE_DIM) for i, val in enumerate(range(1, PUZZLE_DIM**2))}\n",
    "    goal_positions[0] = (PUZZLE_DIM - 1, PUZZLE_DIM - 1)  \n",
    "    manhattan_distance = 0\n",
    "    linear_conflict = 0\n",
    "\n",
    "    for x in range(PUZZLE_DIM):\n",
    "        for y in range(PUZZLE_DIM):\n",
    "            tile = state[x, y]\n",
    "            if tile == 0:\n",
    "                continue\n",
    "            goal_x, goal_y = goal_positions[tile]\n",
    "\n",
    "            manhattan_distance += abs(x - goal_x) + abs(y - goal_y)\n",
    "\n",
    "            # conflicts in rows\n",
    "            if x == goal_x:  \n",
    "                for k in range(y + 1, PUZZLE_DIM):\n",
    "                    conflicting_tile = state[x, k]\n",
    "                    if conflicting_tile != 0 and goal_positions[conflicting_tile][0] == x and goal_positions[conflicting_tile][1] < goal_y:\n",
    "                        linear_conflict += 1\n",
    "\n",
    "            # conflicts in columns\n",
    "            if y == goal_y:  \n",
    "                for k in range(x + 1, PUZZLE_DIM):\n",
    "                    conflicting_tile = state[k, y]\n",
    "                    if conflicting_tile != 0 and goal_positions[conflicting_tile][1] == y and goal_positions[conflicting_tile][0] < goal_x:\n",
    "                        linear_conflict += 1\n",
    "\n",
    "    # each conflict adds 2 moves to the Manhattan distance\n",
    "    return manhattan_distance + 2 * linear_conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A* Search\n",
    "def a_star(state, heuristic):\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (heuristic(state), 0, state.tobytes(), []))  \n",
    "    visited = set()\n",
    "    states_evaluated = 0\n",
    "    while frontier:\n",
    "        _, cost, state_bytes, path = heapq.heappop(frontier) \n",
    "        current_state = np.frombuffer(state_bytes, dtype=int).reshape((PUZZLE_DIM, PUZZLE_DIM))  \n",
    "        states_evaluated += 1\n",
    "        \n",
    "        if goal_test(current_state):  \n",
    "            quality = len(path)\n",
    "            return current_state, path, quality, states_evaluated # final state\n",
    "\n",
    "        visited.add(state_bytes)  # visited state\n",
    "\n",
    "        for action in available_actions(current_state):  # exploring neighbors\n",
    "            new_state = do_action(current_state, action)\n",
    "            new_state_bytes = new_state.tobytes() \n",
    "\n",
    "            if new_state_bytes not in visited:  # avoid revisiting states\n",
    "                new_cost = cost + 1  # g = g + 1\n",
    "                priority = new_cost + heuristic(new_state)  # f = g + h\n",
    "                heapq.heappush(frontier, (priority, new_cost, new_state_bytes, path + [action])) \n",
    "\n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddfc42b66454befb70805bb350ad6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Randomizing:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "[[4 1 8]\n",
      " [7 6 3]\n",
      " [0 2 5]]\n",
      "\n",
      "A* Search with Linear Conflict\n",
      "Steps (quality): 16, Total states evaluated (cost): 52, Final state: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]], A* solution: [Action(pos1=(2, 0), pos2=(1, 0)), Action(pos1=(1, 0), pos2=(0, 0)), Action(pos1=(0, 0), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2)), Action(pos1=(2, 2), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2))]\n",
      "\n",
      "A* Search with Manhattan Distance\n",
      "Steps (quality): 16, Total states evaluated (cost): 90, Final state: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]], A* solution: [Action(pos1=(2, 0), pos2=(1, 0)), Action(pos1=(1, 0), pos2=(0, 0)), Action(pos1=(0, 0), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2)), Action(pos1=(2, 2), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2))]\n",
      "\n",
      "A* Search with Hamming Distance\n",
      "Steps (quality): 16, Total states evaluated (cost): 648, Final state: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]], A* solution: [Action(pos1=(2, 0), pos2=(1, 0)), Action(pos1=(1, 0), pos2=(0, 0)), Action(pos1=(0, 0), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2)), Action(pos1=(2, 2), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2))]\n",
      "\n",
      "Breadth-First Search\n",
      "Steps (quality): 16, Total states evaluated (cost): 7667, Final state: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]], BFS solution: [Action(pos1=(2, 0), pos2=(1, 0)), Action(pos1=(1, 0), pos2=(0, 0)), Action(pos1=(0, 0), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2)), Action(pos1=(2, 2), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2))]\n",
      "\n",
      "Depth-First Search with Bound\n",
      "Searching with depth limit: 1\n",
      "Searching with depth limit: 2\n",
      "Searching with depth limit: 3\n",
      "Searching with depth limit: 4\n",
      "Searching with depth limit: 5\n",
      "Searching with depth limit: 6\n",
      "Searching with depth limit: 7\n",
      "Searching with depth limit: 8\n",
      "Searching with depth limit: 9\n",
      "Searching with depth limit: 10\n",
      "Searching with depth limit: 11\n",
      "Searching with depth limit: 12\n",
      "Searching with depth limit: 13\n",
      "Searching with depth limit: 14\n",
      "Searching with depth limit: 15\n",
      "Steps (quality): 16, Total states evaluated (cost): 33062, Final state: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]], DFS solution: [Action(pos1=(2, 0), pos2=(1, 0)), Action(pos1=(1, 0), pos2=(0, 0)), Action(pos1=(0, 0), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2)), Action(pos1=(2, 2), pos2=(1, 2)), Action(pos1=(1, 2), pos2=(0, 2)), Action(pos1=(0, 2), pos2=(0, 1)), Action(pos1=(0, 1), pos2=(1, 1)), Action(pos1=(1, 1), pos2=(2, 1)), Action(pos1=(2, 1), pos2=(2, 2))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(state)\n",
    "print()\n",
    "print(\"A* Search with Linear Conflict\")\n",
    "final_state_a_star_linear_conflict, path_a_star_linear_conflict, quality_a_star_linear_conflict, cost_a_star_linear_conflict = a_star(state, h_linear_conflict)\n",
    "if path_a_star_linear_conflict: \n",
    "    print(f\"Steps (quality): {quality_a_star_linear_conflict}, Total states evaluated (cost): {cost_a_star_linear_conflict}, Final state: {final_state_a_star_linear_conflict}, A* solution: {path_a_star_linear_conflict}\")\n",
    "else:\n",
    "    print(\"A* linear conflict solution not found\")\n",
    "\n",
    "print()\n",
    "print(\"A* Search with Manhattan Distance\")\n",
    "final_state_a_star_manhattan, path_a_star_manhattan, quality_a_star_manhattan, cost_a_star_manhattan = a_star(state, h_manhattan)\n",
    "if path_a_star_manhattan:\n",
    "    print(f\"Steps (quality): {quality_a_star_manhattan}, Total states evaluated (cost): {cost_a_star_manhattan}, Final state: {final_state_a_star_manhattan}, A* solution: {path_a_star_manhattan}\")\n",
    "else:\n",
    "    print(\"A* manhattan solution not found\")\n",
    "\n",
    "print()\n",
    "print(\"A* Search with Hamming Distance\")\n",
    "final_state_a_star_hamming, path_a_star_hamming, quality_a_star_hamming, cost_a_star_hamming = a_star(state, h_hamming)\n",
    "if path_a_star_hamming:\n",
    "   print(f\"Steps (quality): {quality_a_star_hamming}, Total states evaluated (cost): {cost_a_star_hamming}, Final state: {final_state_a_star_hamming}, A* solution: {path_a_star_hamming}\")\n",
    "else:\n",
    "   print(\"A* hamming solution not found\")\n",
    "\n",
    "print()\n",
    "print(\"Breadth-First Search\")\n",
    "final_state_bfs, path_bfs, quality_bfs, cost_bfs = bfs(state)\n",
    "if path_bfs:\n",
    "    print(f\"Steps (quality): {quality_bfs}, Total states evaluated (cost): {cost_bfs}, Final state: {final_state_bfs}, BFS solution: {path_bfs}\")\n",
    "else:\n",
    "    print(\"BFS solution not found\")\n",
    "\n",
    "print()\n",
    "print(\"Depth-First Search with Bound\")\n",
    "final_state_dfs, path_dfs, quality_dfs, cost_dfs = iterative_deepening_dfs(state)\n",
    "if path_dfs:   \n",
    "    print(f\"Steps (quality): {quality_dfs}, Total states evaluated (cost): {cost_dfs}, Final state: {final_state_dfs}, DFS solution: {path_dfs}\")\n",
    "else:\n",
    "    print(\"DFS solution not found\")\n",
    "\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CI2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
